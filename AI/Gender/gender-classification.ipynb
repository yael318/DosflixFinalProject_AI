{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:#66D9EF; font-size:28px;\">Gender Classification</span><br>\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T07:15:50.085114Z",
     "start_time": "2025-06-06T07:15:43.916529Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install flask-cors\n",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask-cors in c:\\program files\\python311\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: Flask>=0.9 in c:\\program files\\python311\\lib\\site-packages (from flask-cors) (3.0.3)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\program files\\python311\\lib\\site-packages (from Flask>=0.9->flask-cors) (3.0.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\program files\\python311\\lib\\site-packages (from Flask>=0.9->flask-cors) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\program files\\python311\\lib\\site-packages (from Flask>=0.9->flask-cors) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\program files\\python311\\lib\\site-packages (from Flask>=0.9->flask-cors) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\program files\\python311\\lib\\site-packages (from Flask>=0.9->flask-cors) (1.8.2)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from click>=8.1.3->Flask>=0.9->flask-cors) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python311\\lib\\site-packages (from Jinja2>=3.1.2->Flask>=0.9->flask-cors) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Program Files\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\215556754\\Desktop\\Gender\n",
      "2.7.0+cpu\n",
      "0.22.0+cpu\n",
      "2.7.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import os\n",
    "print(os.getcwd())\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision\n",
    "import torchaudio\n",
    "import torchvision.models as models\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"./Gender\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, labels, sp=3):\n",
    "    fig, axes = plt.subplots(sp, sp, figsize=(10, 10))\n",
    "    fig.subplots_adjust(hspace=1, wspace=0.3)\n",
    "\n",
    "    for ax, img_path, label in zip(axes.flatten(), images, labels):\n",
    "        img = mpimg.imread(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.set_xlabel(f'Label: {label}', fontsize=12)\n",
    "        ax.set_title(f'Label: {label}', fontsize=14)\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize image with white background to maintain proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# שינוי גודל תמונה עם רקע לבן לשמירה על פרופורציות\n",
    "def resize_with_white_background(path_ori, path_dest):\n",
    "    img = Image.open(path_ori)\n",
    "    \n",
    "    # שינוי גודל תוך שמירה על יחס גובה-רוחב\n",
    "    img.thumbnail((224, 224), Image.LANCZOS)\n",
    "    \n",
    "    # יצירת תמונה חדשה עם רקע לבן\n",
    "    background = Image.new('RGB', (224, 224), (255, 255, 255))\n",
    "    img_w, img_h = img.size\n",
    "    offset = ((224 - img_w) // 2, (224 - img_h) // 2)\n",
    "    background.paste(img, offset)\n",
    "    \n",
    "    # שמירת הקובץ בתקיית היעד\n",
    "    background.save(os.path.join(\"resized\", path_dest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import face_recognition\n",
    "# from PIL import Image\n",
    "\n",
    "# def crop_face_and_resize(path_ori, path_dest):\n",
    "#     img = face_recognition.load_image_file(path_ori)\n",
    "#     face_locations = face_recognition.face_locations(img)\n",
    "\n",
    "#     if face_locations:\n",
    "#         top, right, bottom, left = face_locations[0]  # ניקח רק את הפנים הראשונות\n",
    "#         face_image = img[top:bottom, left:right]\n",
    "#         pil_image = Image.fromarray(face_image)\n",
    "#     else:\n",
    "#         pil_image = Image.open(path_ori).convert(\"RGB\")  # fallback אם אין פנים\n",
    "\n",
    "#     # שינוי גודל תוך שמירה על יחס גובה-רוחב + רקע לבן\n",
    "#     pil_image.thumbnail((224, 224), Image.LANCZOS)\n",
    "#     background = Image.new('RGB', (224, 224), (255, 255, 255))\n",
    "#     img_w, img_h = pil_image.size\n",
    "#     offset = ((224 - img_w) // 2, (224 - img_h) // 2)\n",
    "#     background.paste(pil_image, offset)\n",
    "    \n",
    "#     background.save(os.path.join(\"resized\", path_dest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ריצה על כל התמונות בתיקייה ושינוי גודלן\n",
    "# for img_path in tqdm(image_paths, desc=\"Resizing Images\"):\n",
    "#     img_name = os.path.basename(img_path)  # חילוץ שם הקובץ\n",
    "#     resize_with_white_background(img_path, img_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#נתיה התיקייה בה התמונות לאחר שינוי הגודל\n",
    "resized_dir = \"./resized\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# איסוף כל התמונות מהתיקייה המעורבת\n",
    "image_paths_resized = []\n",
    "image_labels_resized  = []\n",
    "\n",
    "for file in os.listdir(resized_dir):\n",
    "    if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        # זיהוי מגדר מתוך שם הקובץ\n",
    "        label = \"male\" if file.startswith(\"1\") else \"female\"\n",
    "        image_paths_resized .append(os.path.join(resized_dir, file))\n",
    "        image_labels_resized .append(label)\n",
    "\n",
    "# לבחור 9 תמונות רנדומליות\n",
    "sp = 3  # 3x3 תמונות\n",
    "num_images = min(sp * sp, len(image_paths_resized ))  # לוודא שאין יותר מ-9 תמונות\n",
    "random_indices = np.random.choice(len(image_paths_resized ), num_images, replace=False)\n",
    "selected_paths = [image_paths_resized [i] for i in random_indices]\n",
    "selected_labels = [image_labels_resized [i] for i in random_indices]\n",
    "\n",
    "# קריאה לפונקציה להצגת התמונות\n",
    "# plot_images(selected_paths, selected_labels, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the resized images: (224, 224)\n"
     ]
    }
   ],
   "source": [
    "# confirm the size of the resized images\n",
    "im = Image.open(image_paths_resized[0])\n",
    "resized_size = im.size\n",
    "print(f\"The size of the resized images: {resized_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of male images: 2563\n",
      "Number of female images: 2528\n"
     ]
    }
   ],
   "source": [
    "# Variables to store paths based on gender\n",
    "male_images = []\n",
    "female_images = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for file in os.listdir(resized_dir):\n",
    "    if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        file_path = os.path.join(resized_dir, file)\n",
    "\n",
    "        # Classify based on the beginning of the file name\n",
    "        if file.startswith(\"1\"):\n",
    "            male_images.append(file_path)  # Male\n",
    "        elif file.startswith(\"0\"):\n",
    "            female_images.append(file_path)  # Female\n",
    "\n",
    "# Print the number of files in each category\n",
    "print(f\"Number of male images: {len(male_images)}\")\n",
    "print(f\"Number of female images: {len(female_images)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split to train, validate and test folders with the help of sklearn\n",
    "each split is to 2 groups\n",
    "so we need 2 splits in order to split to 3 groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female_test: 253\n",
      "female_train: 1820\n",
      "female_valid: 455\n"
     ]
    }
   ],
   "source": [
    "# the first split is to female_test and the rest of the images\n",
    "female_train_valid, female_test = train_test_split(female_images, test_size=0.1, random_state=1)\n",
    "\n",
    "# how many female in the test set?\n",
    "print(f\"female_test: {len(female_test)}\")\n",
    "\n",
    "# the second split on the female_train_valid folder separates to 2 folders: train and valid\n",
    "female_train, female_valid = train_test_split(female_train_valid, test_size=0.2, random_state=1)\n",
    "\n",
    "# how many female in the train and valid?\n",
    "print(f\"female_train: {len(female_train)}\")\n",
    "print(f\"female_valid: {len(female_valid)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male_test: 257\n",
      "male_train: 1844\n",
      "male_valid: 462\n"
     ]
    }
   ],
   "source": [
    "# split the male dataset to the same ratio\n",
    "male_train_valid, male_test = train_test_split(male_images, test_size=0.1, random_state=1)\n",
    "\n",
    "male_train, male_valid = train_test_split(male_train_valid, test_size=0.2, random_state=1)\n",
    "print(f\"male_test: {len(male_test)}\")\n",
    "print(f\"male_train: {len(male_train)}\")\n",
    "print(f\"male_valid: {len(male_valid)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./resized\\\\1(1802).jpg',\n",
       " './resized\\\\1(1526).jpg',\n",
       " './resized\\\\1(1633).jpg',\n",
       " './resized\\\\1(937).jpg',\n",
       " './resized\\\\1(1328).jpg',\n",
       " './resized\\\\1(1250).jpg',\n",
       " './resized\\\\1(1528).jpg',\n",
       " './resized\\\\1(2561).jpg',\n",
       " './resized\\\\1(247).jpg',\n",
       " './resized\\\\1(897).jpg',\n",
       " './resized\\\\1(2025).jpg',\n",
       " './resized\\\\1(1505).jpg',\n",
       " './resized\\\\1(749).jpg',\n",
       " './resized\\\\1(1181).jpg',\n",
       " './resized\\\\1(354).jpg',\n",
       " './resized\\\\1(1593).jpg',\n",
       " './resized\\\\1(1266).jpg',\n",
       " './resized\\\\1(1003).jpg',\n",
       " './resized\\\\1(2505).jpg',\n",
       " './resized\\\\1(1823).jpg',\n",
       " './resized\\\\1(1416).jpg',\n",
       " './resized\\\\1(564).jpg',\n",
       " './resized\\\\1(1001).jpg',\n",
       " './resized\\\\1(803).jpg',\n",
       " './resized\\\\1(1272).jpg',\n",
       " './resized\\\\1(2408).jpg',\n",
       " './resized\\\\1(1909).jpg',\n",
       " './resized\\\\1(2110).jpg',\n",
       " './resized\\\\1(1281).jpg',\n",
       " './resized\\\\1(2151).jpg',\n",
       " './resized\\\\1(155).jpg',\n",
       " './resized\\\\1(1545).jpg',\n",
       " './resized\\\\1(1605).jpg',\n",
       " './resized\\\\1(1830).jpg',\n",
       " './resized\\\\1(39).jpg',\n",
       " './resized\\\\1(1090).jpg',\n",
       " './resized\\\\1(2011).jpg',\n",
       " './resized\\\\1(1329).jpg',\n",
       " './resized\\\\1(2547).jpg',\n",
       " './resized\\\\1(1741).jpg',\n",
       " './resized\\\\1(220).jpg',\n",
       " './resized\\\\1(1468).jpg',\n",
       " './resized\\\\1(922).jpg',\n",
       " './resized\\\\1(911).jpg',\n",
       " './resized\\\\1(1206).jpg',\n",
       " './resized\\\\1(1868).jpg',\n",
       " './resized\\\\1(1588).jpg',\n",
       " './resized\\\\1(520).jpg',\n",
       " './resized\\\\1(1622).jpg',\n",
       " './resized\\\\1(2354).jpg',\n",
       " './resized\\\\1(2001).jpg',\n",
       " './resized\\\\1(1537).jpg',\n",
       " './resized\\\\1(1859).jpg',\n",
       " './resized\\\\1(1551).jpg',\n",
       " './resized\\\\1(2555).jpg',\n",
       " './resized\\\\1(761).jpg',\n",
       " './resized\\\\1(1874).jpg',\n",
       " './resized\\\\1(624).jpg',\n",
       " './resized\\\\1(1809).jpg',\n",
       " './resized\\\\1(1043).jpg',\n",
       " './resized\\\\1(1518).jpg',\n",
       " './resized\\\\1(1683).jpg',\n",
       " './resized\\\\1(169).jpg',\n",
       " './resized\\\\1(1095).jpg',\n",
       " './resized\\\\1(1034).jpg',\n",
       " './resized\\\\1(2415).jpg',\n",
       " './resized\\\\1(193).jpg',\n",
       " './resized\\\\1(2490).jpg',\n",
       " './resized\\\\1(669).jpg',\n",
       " './resized\\\\1(2366).jpg',\n",
       " './resized\\\\1(837).jpg',\n",
       " './resized\\\\1(1527).png',\n",
       " './resized\\\\1(1883).jpg',\n",
       " './resized\\\\1(777).jpg',\n",
       " './resized\\\\1(1174).jpg',\n",
       " './resized\\\\1(799).jpg',\n",
       " './resized\\\\1(782).jpg',\n",
       " './resized\\\\1(1485).jpg',\n",
       " './resized\\\\1(784).jpg',\n",
       " './resized\\\\1(1543).jpg',\n",
       " './resized\\\\1(2559).jpg',\n",
       " './resized\\\\1(1430).jpg',\n",
       " './resized\\\\1(1183).jpg',\n",
       " './resized\\\\1(734).jpeg',\n",
       " './resized\\\\1(1246).jpg',\n",
       " './resized\\\\1(208).jpg',\n",
       " './resized\\\\1(1579).jpg',\n",
       " './resized\\\\1(87).jpg',\n",
       " './resized\\\\1(571).jpeg',\n",
       " './resized\\\\1(2327).jpg',\n",
       " './resized\\\\1(1577).jpg',\n",
       " './resized\\\\1(532).jpg',\n",
       " './resized\\\\1(1744).jpg',\n",
       " './resized\\\\1(1071).jpg',\n",
       " './resized\\\\1(1814).jpg',\n",
       " './resized\\\\1(1907).jpg',\n",
       " './resized\\\\1(2283).jpg',\n",
       " './resized\\\\1(2221).jpg',\n",
       " './resized\\\\1(597).jpg',\n",
       " './resized\\\\1(2189).jpg',\n",
       " './resized\\\\1(2075).jpg',\n",
       " './resized\\\\1(2262).jpg',\n",
       " './resized\\\\1(813).jpg',\n",
       " './resized\\\\1(2041).jpg',\n",
       " './resized\\\\1(1693).jpg',\n",
       " './resized\\\\1(678).jpg',\n",
       " './resized\\\\1(2176).jpg',\n",
       " './resized\\\\1(573).jpg',\n",
       " './resized\\\\1(250).jpg',\n",
       " './resized\\\\1(1659).jpg',\n",
       " './resized\\\\1(1437).jpg',\n",
       " './resized\\\\1(2132).jpg',\n",
       " './resized\\\\1(217).jpg',\n",
       " './resized\\\\1(2239).jpg',\n",
       " './resized\\\\1(1589).jpg',\n",
       " './resized\\\\1(2156).jpg',\n",
       " './resized\\\\1(1158).jpg',\n",
       " './resized\\\\1(2027).jpg',\n",
       " './resized\\\\1(1288).jpg',\n",
       " './resized\\\\1(1372).jpg',\n",
       " './resized\\\\1(2454).jpg',\n",
       " './resized\\\\1(1755).jpg',\n",
       " './resized\\\\1(1722).jpg',\n",
       " './resized\\\\1(570).jpg',\n",
       " './resized\\\\1(924).jpg',\n",
       " './resized\\\\1(194).png',\n",
       " './resized\\\\1(1252).jpg',\n",
       " './resized\\\\1(1733).jpg',\n",
       " './resized\\\\1(447).jpg',\n",
       " './resized\\\\1(1978).jpg',\n",
       " './resized\\\\1(2477).jpg',\n",
       " './resized\\\\1(1928).jpg',\n",
       " './resized\\\\1(1397).jpg',\n",
       " './resized\\\\1(375).jpg',\n",
       " './resized\\\\1(592).jpg',\n",
       " './resized\\\\1(1947).jpg',\n",
       " './resized\\\\1(2211).jpg',\n",
       " './resized\\\\1(282).jpg',\n",
       " './resized\\\\1(2234).jpg',\n",
       " './resized\\\\1(1057).jpg',\n",
       " './resized\\\\1(1673).jpg',\n",
       " './resized\\\\1(1751).jpg',\n",
       " './resized\\\\1(360).jpg',\n",
       " './resized\\\\1(1393).jpg',\n",
       " './resized\\\\1(1274).jpg',\n",
       " './resized\\\\1(1255).jpg',\n",
       " './resized\\\\1(989).jpg',\n",
       " './resized\\\\1(2496).jpg',\n",
       " './resized\\\\1(558).jpg',\n",
       " './resized\\\\1(1261).jpg',\n",
       " './resized\\\\1(204).png',\n",
       " './resized\\\\1(972).jpg',\n",
       " './resized\\\\1(1726).jpg',\n",
       " './resized\\\\1(524).jpg',\n",
       " './resized\\\\1(1720).jpg',\n",
       " './resized\\\\1(281).jpg',\n",
       " './resized\\\\1(676).jpg',\n",
       " './resized\\\\1(706).jpg',\n",
       " './resized\\\\1(2503).jpg',\n",
       " './resized\\\\1(1975).jpg',\n",
       " './resized\\\\1(1672).jpg',\n",
       " './resized\\\\1(1502).jpg',\n",
       " './resized\\\\1(1203).jpg',\n",
       " './resized\\\\1(1573).jpg',\n",
       " './resized\\\\1(550).jpg',\n",
       " './resized\\\\1(2350).jpg',\n",
       " './resized\\\\1(1152).jpg',\n",
       " './resized\\\\1(2296).jpg',\n",
       " './resized\\\\1(821).jpg',\n",
       " './resized\\\\1(252).jpg',\n",
       " './resized\\\\1(2016).jpg',\n",
       " './resized\\\\1(2251).jpg',\n",
       " './resized\\\\1(565).jpg',\n",
       " './resized\\\\1(2419).jpg',\n",
       " './resized\\\\1(2073).jpg',\n",
       " './resized\\\\1(1257).png',\n",
       " './resized\\\\1(2101).jpg',\n",
       " './resized\\\\1(2045).jpg',\n",
       " './resized\\\\1(2008).jpg',\n",
       " './resized\\\\1(2242).jpg',\n",
       " './resized\\\\1(158).jpg',\n",
       " './resized\\\\1(938).jpg',\n",
       " './resized\\\\1(1754).jpeg',\n",
       " './resized\\\\1(1539).jpg',\n",
       " './resized\\\\1(1690).jpg',\n",
       " './resized\\\\1(62).jpg',\n",
       " './resized\\\\1(673).jpg',\n",
       " './resized\\\\1(1772).jpg',\n",
       " './resized\\\\1(1730).jpg',\n",
       " './resized\\\\1(1096).jpg',\n",
       " './resized\\\\1(2538).jpg',\n",
       " './resized\\\\1(230).jpg',\n",
       " './resized\\\\1(1631).jpg',\n",
       " './resized\\\\1(1013).jpg',\n",
       " './resized\\\\1(2539).jpg',\n",
       " './resized\\\\1(55).jpg',\n",
       " './resized\\\\1(2044).jpg',\n",
       " './resized\\\\1(996).jpg',\n",
       " './resized\\\\1(2257).jpg',\n",
       " './resized\\\\1(1888).jpg',\n",
       " './resized\\\\1(2427).jpg',\n",
       " './resized\\\\1(715).jpg',\n",
       " './resized\\\\1(976).jpg',\n",
       " './resized\\\\1(1361).jpg',\n",
       " './resized\\\\1(355).jpg',\n",
       " './resized\\\\1(1597).jpg',\n",
       " './resized\\\\1(1621).jpg',\n",
       " './resized\\\\1(2432).jpg',\n",
       " './resized\\\\1(587).jpg',\n",
       " './resized\\\\1(929).jpg',\n",
       " './resized\\\\1(506).jpg',\n",
       " './resized\\\\1(530).jpg',\n",
       " './resized\\\\1(789).jpg',\n",
       " './resized\\\\1(812).jpg',\n",
       " './resized\\\\1(1461).jpg',\n",
       " './resized\\\\1(852).jpg',\n",
       " './resized\\\\1(1448).jpg',\n",
       " './resized\\\\1(1568).jpg',\n",
       " './resized\\\\1(567).jpg',\n",
       " './resized\\\\1(973).jpg',\n",
       " './resized\\\\1(2495).jpg',\n",
       " './resized\\\\1(1984).jpg',\n",
       " './resized\\\\1(164).jpg',\n",
       " './resized\\\\1(1800).jpg',\n",
       " './resized\\\\1(1106).jpg',\n",
       " './resized\\\\1(1168).jpg',\n",
       " './resized\\\\1(2136).jpg',\n",
       " './resized\\\\1(1356).jpg',\n",
       " './resized\\\\1(2102).jpg',\n",
       " './resized\\\\1(226).jpg',\n",
       " './resized\\\\1(2039).png',\n",
       " './resized\\\\1(2417).jpg',\n",
       " './resized\\\\1(309).jpg',\n",
       " './resized\\\\1(263).jpg',\n",
       " './resized\\\\1(2140).jpg',\n",
       " './resized\\\\1(1825).jpg',\n",
       " './resized\\\\1(1428).jpg',\n",
       " './resized\\\\1(891).jpg',\n",
       " './resized\\\\1(1490).jpg',\n",
       " './resized\\\\1(239).jpg',\n",
       " './resized\\\\1(2160).jpg',\n",
       " './resized\\\\1(999).jpg',\n",
       " './resized\\\\1(2351).jpg',\n",
       " './resized\\\\1(1656).jpg',\n",
       " './resized\\\\1(1651).jpg',\n",
       " './resized\\\\1(2088).jpg',\n",
       " './resized\\\\1(84).jpg',\n",
       " './resized\\\\1(807).jpg',\n",
       " './resized\\\\1(770).jpg',\n",
       " './resized\\\\1(404).jpg',\n",
       " './resized\\\\1(1431).jpg',\n",
       " './resized\\\\1(2241).jpg',\n",
       " './resized\\\\1(184).jpg',\n",
       " './resized\\\\1(965).jpg',\n",
       " './resized\\\\1(323).jpg',\n",
       " './resized\\\\1(825).jpg',\n",
       " './resized\\\\1(1105).jpeg']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame נמיר את הרשימות ל   \n",
    "\n",
    "female_train = pd.DataFrame(female_train, columns=['filename'])\n",
    "female_test = pd.DataFrame(female_test, columns=['filename'])\n",
    "female_valid = pd.DataFrame(female_valid, columns=['filename'])\n",
    "male_train = pd.DataFrame(male_train, columns=['filename'])\n",
    "male_test = pd.DataFrame(male_test, columns=['filename'])\n",
    "male_valid = pd.DataFrame(male_valid, columns=['filename'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset by distributing the images to 3 folders: train, validate, test\n",
    "# each containing 2 subfolders: female and male\n",
    "for item in female_train['filename']:\n",
    "  source = os.path.join(\"resized\", os.path.basename(item))\n",
    "  dest= 'dataset/train/female/' + os.path.basename(item)\n",
    "  shutil.copy(source, dest)\n",
    "\n",
    "for item in female_test['filename']:\n",
    "  source = os.path.join(\"resized\", os.path.basename(item))\n",
    "  dest= 'dataset/test/female/' + os.path.basename(item)\n",
    "  shutil.copy(source, dest)\n",
    "\n",
    "for item in female_valid['filename']:\n",
    "  source = os.path.join(\"resized\", os.path.basename(item))\n",
    "  dest= 'dataset/validate/female/' + os.path.basename(item)\n",
    "  shutil.copy(source, dest)\n",
    "\n",
    "\n",
    "for item in male_train['filename']:\n",
    "  source = os.path.join(\"resized\", os.path.basename(item))\n",
    "  dest= 'dataset/train/male/' + os.path.basename(item)\n",
    "  shutil.copy(source, dest)\n",
    "\n",
    "for item in male_test['filename']:\n",
    "  source = os.path.join(\"resized\", os.path.basename(item))\n",
    "  dest= 'dataset/test/male/' + os.path.basename(item)\n",
    "  shutil.copy(source, dest)\n",
    "\n",
    "for item in male_valid['filename']:\n",
    "  source = os.path.join(\"resized\", os.path.basename(item))\n",
    "  dest= 'dataset/validate/male/' + os.path.basename(item)\n",
    "  shutil.copy(source, dest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to the datasets\n",
    "train_path = 'dataset/train/'\n",
    "valid_path = 'dataset/validate/'\n",
    "test_path  = 'dataset/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# הגדרת transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder('dataset/train', transform=train_transforms)\n",
    "val_dataset   = datasets.ImageFolder('dataset/validate', transform=val_test_transforms)\n",
    "test_dataset  = datasets.ImageFolder('dataset/test',  transform=val_test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape)  # צפוי: torch.Size([32, 3, 224, 224])\n",
    "print(labels.shape)  # צפוי: torch.Size([32])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128, 1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # eval על val_loader...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9431\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total, correct = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        preds = (torch.sigmoid(outputs).squeeze() > 0.5).int()\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "print(f\"Test Accuracy: {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"my_model.pt\")\n",
    "# model.save('my_model.h5')  # או לתיקייה: model.save('my_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
